{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune T5 locally for machine translation on COVID-19 Health Service Announcements with Hugging Face\n",
    "\n",
    "[![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/aws/studio-lab-examples/blob/main/natural-language-processing/NLP_Disaster_Recovery_Translation.ipynb)\n",
    "\n",
    "This notebook is designed to run within SageMaker Lab, on a `g4dn.xlarge` GPU instance. If you are not using that right now, please restart your session and select `GPU`, as this will help you train your model in a matter of tens of minutes, rather than hours.\n",
    "\n",
    "If you are ready for training a large-scale machine translation model, then please check out using Hugging Face on Amazon SageMaker! \n",
    "\n",
    "Otherwise, please enjoy this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0. Install all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "ipywidgets\n",
    "git+https://github.com/huggingface/transformers\n",
    "datasets\n",
    "sacrebleu\n",
    "torch\n",
    "sentencepiece\n",
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers (from -r requirements.txt (line 3))\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-xq2jzeya\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-xq2jzeya\n",
      "  Resolved https://github.com/huggingface/transformers to commit 835de4c8335f72a9c53178f54cc3b4c0688960ec\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ipywidgets in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (8.1.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (2.18.0)\n",
      "Collecting sacrebleu (from -r requirements.txt (line 5))\n",
      "  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (2.0.0.post200)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.1.99)\n",
      "Requirement already satisfied: evaluate in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.4.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 2)) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 2)) (8.22.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 2)) (5.14.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 2)) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 2)) (3.0.10)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from transformers==4.41.0.dev0->-r requirements.txt (line 3)) (3.13.3)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers==4.41.0.dev0->-r requirements.txt (line 3))\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from transformers==4.41.0.dev0->-r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from transformers==4.41.0.dev0->-r requirements.txt (line 3)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from transformers==4.41.0.dev0->-r requirements.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from transformers==4.41.0.dev0->-r requirements.txt (line 3)) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from transformers==4.41.0.dev0->-r requirements.txt (line 3)) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.0.dev0->-r requirements.txt (line 3))\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from transformers==4.41.0.dev0->-r requirements.txt (line 3)) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from transformers==4.41.0.dev0->-r requirements.txt (line 3)) (4.66.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 4)) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 4)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 4)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 4)) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 4)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 4)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 4)) (3.9.3)\n",
      "Collecting portalocker (from sacrebleu->-r requirements.txt (line 5))\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from sacrebleu->-r requirements.txt (line 5)) (0.9.0)\n",
      "Requirement already satisfied: colorama in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from sacrebleu->-r requirements.txt (line 5)) (0.4.6)\n",
      "Requirement already satisfied: lxml in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from sacrebleu->-r requirements.txt (line 5)) (4.9.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from torch->-r requirements.txt (line 6)) (3.1.3)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 8)) (0.18.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (4.0.3)\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (3.0.42)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (0.6.2)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from requests->transformers==4.41.0.dev0->-r requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from requests->transformers==4.41.0.dev0->-r requirements.txt (line 3)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from requests->transformers==4.41.0.dev0->-r requirements.txt (line 3)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from requests->transformers==4.41.0.dev0->-r requirements.txt (line 3)) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 6)) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 4)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 4)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 4)) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 2)) (0.2.2)\n",
      "Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.41.0.dev0-py3-none-any.whl size=9046150 sha256=7c4417ace57ac83bcb24e12c408fc213da2ea687387497c84ac141ce5acfcfed\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-o5qa2tw1/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
      "Successfully built transformers\n",
      "Installing collected packages: portalocker, sacrebleu, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface_hub 0.22.2\n",
      "    Uninstalling huggingface_hub-0.22.2:\n",
      "      Successfully uninstalled huggingface_hub-0.22.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.31.0\n",
      "    Uninstalling transformers-4.31.0:\n",
      "      Successfully uninstalled transformers-4.31.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 0.8.2 requires transformers[sentencepiece]<4.32.0,>=4.31.0, but you have transformers 4.41.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.23.0 portalocker-2.8.2 sacrebleu-2.4.2 tokenizers-0.19.1 transformers-4.41.0.dev0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "# make sure to restart your kernel to use the newly install packages\n",
    "# IPython.Application.instance().kernel.do_shutdown(True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Explore the available datasets on Translators without Borders \n",
    "Then, download a pair you would like to use for training a language translation model. The steps below download the translation pairs for English to Spanish, but you are welcome to modify these and use a different pair if you prefer.\n",
    "\n",
    "Overall site page: https://tico-19.github.io/\n",
    "\n",
    "Page with all language pairs: https://tico-19.github.io/memories.html \n",
    "\n",
    "Scroll through all supported language pairs and pick your favorite. We'll demonstrate English to Spanish, `en-to-es`\n",
    "\n",
    "Copy the link to that pair, for `en-to-es` it looks like this:\n",
    "- https://tico-19.github.io/data/TM/all.en-es-LA.tmx.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_my_data = 'https://tico-19.github.io/data/TM/all.en-es-LA.tmx.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget {path_to_my_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file = path_to_my_data.split('/')[-1]\n",
    "print (local_file)\n",
    "filename = local_file.split('.zip')[0]\n",
    "print (filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip {local_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extract data from `.tmx` file type \n",
    "Next, you can use this local function to extract data from the `.tmx` file type and format for local training with Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste the name of your file and language codes here\n",
    "source_code_1 = 'en'\n",
    "target_code_2 =  'es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tmx(filename, source_code_1, target_code_2):\n",
    "    '''\n",
    "    Takes a local TMX filename and codes for source and target languages. \n",
    "    Walks through your file, row by row, looking for tmx / html specific formatting.\n",
    "    If there's a regex match, will clean your string and add to a dictionary for downstream pandas formatting.\n",
    "    '''\n",
    "    \n",
    "    data = {source_code_1:[], target_code_2:[]}\n",
    "\n",
    "    with open(filename) as f:\n",
    "\n",
    "        for row in f.readlines():\n",
    "\n",
    "            if not row.endswith('</seg></tuv>\\n'):\n",
    "                continue\n",
    "\n",
    "            if row.startswith('<seg>'):\n",
    "\n",
    "                st_1 = row.strip()\n",
    "\n",
    "                st_1 = st_1.replace('<seg>', '')\n",
    "                st_1 = st_1.replace('</seg></tuv>', '')\n",
    "\n",
    "                data[source_code_1].append(st_1)\n",
    "\n",
    "            # when you use your own target code, remove the -LA string \n",
    "            if row.startswith('<tuv xml:lang=\"{}-LA\"><seg>'.format(target_code_2)):\n",
    "\n",
    "                st_2 = row.strip()\n",
    "                # when you use your own target code, remove the -LA string \n",
    "                st_2 = st_2.replace('<tuv xml:lang=\"{}-LA\"><seg>'.format(target_code_2), '')\n",
    "                st_2 = st_2.replace('</seg></tuv>', '')\n",
    "\n",
    "                data[target_code_2].append(st_2)\n",
    "                \n",
    "        return data\n",
    "\n",
    "data = parse_tmx(filename, source_code_1, target_code_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this makes sure you got actual pairs\n",
    "assert len(data[source_code_1]) == len(data[target_code_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(data, orient = 'columns')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to disk in case you need to restart your kernel later\n",
    "df.to_csv('language_pairs.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Format extracted data for machine translation with Hugging Face\n",
    "Core examples available right here: https://github.com/huggingface/transformers/tree/master/examples/pytorch/translation \n",
    "\n",
    "Guidance on formatting for Hugging Face datasets here:\n",
    "https://huggingface.co/docs/datasets/loading_datasets.html#json-files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('language_pairs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of translation supports only custom JSONLINES files, with each line being a dictionary with a key \"translation\" and its value another dictionary whose keys is the language pair. For example:\n",
    "\n",
    "`{ \"translation\": { \"en\": \"Others have dismissed him as a joke.\", \"ro\": \"Alții l-au numit o glumă.\" } }\n",
    "{ \"translation\": { \"en\": \"And some are holding out for an implosion.\", \"ro\": \"Iar alții așteaptă implozia.\" } }`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    \n",
    "    obj = {\"translation\": {source_code_1: row[source_code_1], target_code_2: row[target_code_2]}} \n",
    "    objs.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "!mkdir data\n",
    "with open('data/train.json', 'w') as f:\n",
    "    for row in objs:\n",
    "        j = json.dumps(row, ensure_ascii = False)\n",
    "        f.write(j)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Finetune a machine translation model locally\n",
    "Do to this, let's first download the raw Python file we need from Hugging Face to finetune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/translation/run_translation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# full hugging face Trainer API args available here\n",
    "# https://github.com/huggingface/transformers/blob/de635af3f1ef740aa32f53a91473269c6435e19e/src/transformers/training_args.py\n",
    "# T5 trainig args available here\n",
    "# https://huggingface.co/transformers/model_doc/t5.html#t5config\n",
    "!python run_translation.py \\\n",
    "    --model_name_or_path t5-small \\\n",
    "    --do_train \\\n",
    "    --source_lang en \\\n",
    "    --target_lang es \\\n",
    "    --source_prefix \"translate English to Spanish: \" \\\n",
    "    --train_file data/train.json \\\n",
    "    --output_dir output/tst-translation \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --per_device_eval_batch_size=4 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate \\\n",
    "    --save_strategy epoch \\\n",
    "    --num_train_epochs 3\n",
    "#     --do_eval \\\n",
    "#     --validation_file path_to_jsonlines_file \\\n",
    "#     --dataset_name cov-19 \\\n",
    "#     --dataset_config_name en-es \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls output/tst-translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Test your newly fine-tuned translation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(pretrained_model_name_or_path = 'output/tst-translation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# line to make sure your model supports local inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's test it! Remember that, in using the default settings of only 3 epoch, your translation is probably not going to be SOTA. For achieving state of the art, (SOTA), we recommend migrating to Amazon SageMaker to scale up and out. Scaling up means moving your code to a more advanced compute type, such as a p4 series or even Trainium. Scaling out means adding more compute, so going from 1 to many instances. Using the entire AWS cloud you can train for much longer periods of time on much larger datasets, which can directly translate to a more accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = ['about how long have these symptoms been going on?',\t\n",
    "'and all chest pain should be treated this way especially with your age\t',\n",
    "'and along with a fever\t',\n",
    "'and also needs to be checked your cholesterol blood pressure',\t\n",
    "'and are you having a fever now?\t',\n",
    "'and are you having any of the following symptoms with your chest pain',\t\n",
    "'and are you having a runny nose?',\t\n",
    "'and are you having this chest pain now?',\n",
    "'and besides do you have difficulty breathing',\n",
    "'and can you tell me what other symptoms are you having along with this?',\n",
    "'and does this pain move from your chest?',\n",
    "'and drink lots of fluids',\n",
    "'and how high has your fever been',\n",
    "'and i have a cough too',\n",
    "'and i have a little cold and a cough',\n",
    "'''and i'm really having some bad chest pain today''']\n",
    "\n",
    "task_prefix = \"translate English to Spanish: \"\n",
    "\n",
    "for i in input_sequences:\n",
    "    input_ids = tokenizer('''{} {}'''.format(task_prefix, i), return_tensors='pt').input_ids\n",
    "    outputs = model.generate(input_ids)\n",
    "    print(i, tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('my-tf-en-to-sp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czf my_model.tar.gz my-tf-en-to-sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
